{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "web_scraping.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN0rh6uaF8I3QJG0DI7Cdz3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vanviethieuanh/CS114.L21/blob/main/News%20Scraping/19522054/news/web_scraping.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pIrx524dHqs9"
      },
      "source": [
        "**WEB SCRAPING**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W1Os3isVH09s"
      },
      "source": [
        "**1. Link các trang báo đã lấy dữ liệu:**\n",
        "+ Trang chính thống: https://www.theaustralian.com.au/\n",
        "+ Trang chấm biếm: https://thehardtimes.net/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EJbDarzRIiQM"
      },
      "source": [
        "**2. Tài liệu tham khảo:**\n",
        "\n",
        " + Python Crawl Dữ Liệu: https://codelearn.io/sharing/python-crawl-du-lieu-tao-bai-bao-nhanh\n",
        "\n",
        " + Beautiful Soup Documentation: https://www.crummy.com/software/BeautifulSoup/bs4/doc/\n",
        "\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QZUkVBb7Ir7U"
      },
      "source": [
        "**3. Code:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y96NvWY-KzqL"
      },
      "source": [
        "+ Import thư viện cần thiết"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6hx6KtFUHctK"
      },
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from pandas import DataFrame\n",
        "import numpy as np\n",
        "import json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cFYxtsypK5E7"
      },
      "source": [
        "+ Lấy dữ liệu từ trang web về, lưu vào list tạm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FeTvBx1hJPg7"
      },
      "source": [
        "# Khởi tạo list lưu trữ dữ liệu\n",
        "list_link = []\n",
        "list_headline = []\n",
        "list_date = []\n",
        "\n",
        "# Số trang cần crawl dữ liệu\n",
        "n = 123\n",
        "\n",
        "# Số bài báo cần lấy trong mỗi trang\n",
        "m = 10\n",
        "\n",
        "for i in range(1, n):\n",
        "    # Link trang tờ báo\n",
        "    if i == 1:\n",
        "        url = 'https://www.theaustralian.com.au/life/style'\n",
        "    else:\n",
        "        url = 'https://www.theaustralian.com.au/life/style/page/{0}'.format(i)\n",
        "\n",
        "    # Lấy dữ liệu thô từ trang web\n",
        "    response = requests.get(url)\n",
        "\n",
        "    # Tách dữ liệu\n",
        "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "\n",
        "    # Phân tích dữ liệu\n",
        "    titles = soup.findAll('h3', class_='story-block__heading')\n",
        "    links = [link.find('a').attrs[\"href\"] for link in titles]\n",
        "    headlines = [headline.find('a').text for headline in titles]\n",
        "    publication_dates = soup.findAll('span', class_='show-for-xlarge')\n",
        "    dates = [date.text for date in publication_dates]\n",
        "\n",
        "    # Lấy dữ liệu từ mỗi trang vào list lưu trữ tạm thời\n",
        "    list_link.extend(links[0:m])\n",
        "    list_headline.extend(headlines[0:m])\n",
        "    list_date.extend(dates[0:m])\n",
        "    print('\\n', i)\n",
        "\n",
        "    # Show ra trang báo cuối cùng để check ngày tháng và các trường dữ liệu có khớp nhau không?\n",
        "    if i == n-1:\n",
        "        for i in range(m):\n",
        "            print(\"article_link: \", links[i])\n",
        "            print(\"headline: \", headlines[i])\n",
        "            print('post_at: ', dates[i])\n",
        "            print(\"is_sarcastic: 0\")\n",
        "            print(\"_________________________________________________________________________\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZYy5aF1CLxBU"
      },
      "source": [
        "+ Lưu dữ liệu vừa lấy vào file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sJ4k_0Y_KNOT"
      },
      "source": [
        "# Kiểm tra các trường dữ liệu có bằng nhau hay không? Rồi lưu dữ liệu vào file csv\n",
        "if len(list_link) == len(list_headline) and len(list_headline) == len(list_date):\n",
        "    list_ones = np.zeros(len(list_date), dtype=int)\n",
        "\n",
        "    data = {\n",
        "        \"article_link\": list_link,\n",
        "        \"headline\": list_headline,\n",
        "        \"post_at\": list_date,\n",
        "        \"is_sarcastic\": list_ones\n",
        "    }\n",
        "    df = DataFrame(data, columns= ['article_link', 'headline', 'post_at', 'is_sarcastic'])\n",
        "    export_csv = df.to_csv (r'/content/drive/MyDrive/Colab Notebooks/Sarcasm_Detection/data_1_1.csv', index = None, header=True)\n",
        "    print(df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5bBJHuq1L8Rz"
      },
      "source": [
        "+ Tổng hợp dữ liệu từ nhiều file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CN_nYiLLMDfc"
      },
      "source": [
        "# Khởi tạo list lưu trữ dữ liệu\n",
        "list_links = []\n",
        "list_headlines = []\n",
        "list_dates = []\n",
        "list_ones = []\n",
        "\n",
        "# Số file dữ liệu\n",
        "n = 18\n",
        "\n",
        "# Lấy dữ liệu từ mỗi file\n",
        "for i in range(1, n+1):\n",
        "    # Đọc dữ liệu từ file\n",
        "    df = pd.read_csv('data_1_' + str(i) + '.csv')\n",
        "\n",
        "    # Thêm dữ liệu từ file vào list lưu trữ dữ liệu\n",
        "    list_links.extend(df['article_link'])\n",
        "    list_headlines.extend(df['headline'])\n",
        "    list_dates.extend(df['post_at'])\n",
        "\n",
        "# Lấy list các số biểu thị cho tờ báo chính thống (số 0)\n",
        "list_ones = np.zeros(len(list_dates), dtype=int)\n",
        "\n",
        "# Lưu dữ liệu vào dictionayry\n",
        "data = {\n",
        "    \"article_link\": list_links,\n",
        "    \"headline\": list_headlines,\n",
        "    \"post_at\": list_dates,\n",
        "    \"is_sarcastic\": list_ones\n",
        "}\n",
        "df = DataFrame(data, columns= ['article_link', 'headline', 'post_at', 'is_sarcastic'])\n",
        "export_csv = df.to_csv (r'/content/drive/MyDrive/Colab Notebooks/Sarcasm_Detection/data_0.csv', index = None, header=True)\n",
        "print(df)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}